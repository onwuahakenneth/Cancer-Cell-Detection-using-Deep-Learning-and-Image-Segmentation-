{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2079,"status":"ok","timestamp":1725910613502,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"27pAPmR0KTCR","outputId":"a9ab9b2c-ea05-4f0f-b90d-ae8f2d1bc7be"},"outputs":[],"source":["# Execute if running on Colab\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4285,"status":"ok","timestamp":1725910617783,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"kYSOTAyWMtQj"},"outputs":[],"source":["import os\n","import cv2\n","import pickle\n","import pathlib\n","import numpy as np\n","import tensorflow as tf\n","from sklearn import utils\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","from numpy.typing import NDArray\n","import xml.etree.ElementTree as ET\n","from warnings import filterwarnings\n","from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress Tensorflow messages\n","filterwarnings(\"ignore\")\n","%matplotlib inline\n","\n","# ##################################\n","# Global constants - amend as needed\n","# ##################################\n","\n","# Set Root Directory...\n","# Google Drive\n","BASE_FOLDER = '/content/drive/My Drive/A.I. Group 6 Project'\n","\n","DATASET_FOLDER = f'{BASE_FOLDER}/Datasets/CTC_CCA_dataset'\n","MODEL_FOLDER = f'{BASE_FOLDER}/Models'\n","ANNOTATION_FOLDER = f'{DATASET_FOLDER}/annotations'\n","ANNOTATION_TRAIN_FOLDER = f'{ANNOTATION_FOLDER}/trainval'\n","ANNOTATION_TEST_FOLDER = f'{ANNOTATION_FOLDER}/test'\n","IMG_FOLDER = f'{DATASET_FOLDER}/raw_images_for_model'                            # 1207 images - we use only 1087\n","IMG_BF_FOLDER = f'{IMG_FOLDER}/brightfield'\n","IMG_FLR_FOLDER = f'{IMG_FOLDER}/fluorescence'\n","CELL_IMG_FOLDER = f'{DATASET_FOLDER}/cell_images'\n","CELL_IMG_TRAIN_FOLDER =f'{CELL_IMG_FOLDER}/trainval/fluorescence'\n","CELL_IMG_TEST_FOLDER =f'{CELL_IMG_FOLDER}/test/fluorescence'\n","\n","IMG_RESIZE = 224\n","VALIDATION_SPLIT = 0.15\n","EPOCHS=10\n","BATCH_SIZE = 32\n","\n","# set constant to zero if want to process all the images\n","IMG_TRAIN_CNT = 0               # includes validation\n","IMG_TEST_CNT = 0\n","CELL_CLS_TRAIN_CNT = 3000       # per class, includes validation\n","CELL_CLS_TEST_CNT = 600         # per class\n","\n","# cell model - transfer model trained with cell images\n","# segmentation model - weights and results using transfer models\n","CREATE_CELL_IMAGES = False\n","RETRAIN_CELL_MODEL = False\n","RETRAIN_IMG_MODEL = False\n","model_info = {\n","    'cell_train_weights'         : f'{MODEL_FOLDER}/cell_train.weights.h5',\n","    'cell_train_results'         : f'{MODEL_FOLDER}/cell_train.results.pkl',\n","    'img_train_nontuned_weights' : f'{MODEL_FOLDER}/img_train_nontuned.weights.h5',\n","    'img_train_nontuned_results' : f'{MODEL_FOLDER}/img_train_nontuned.results.pkl',\n","    'img_train_tuned_weights'    : f'{MODEL_FOLDER}/img_train_tuned.weights.h5',\n","    'img_train_tuned_results'    : f'{MODEL_FOLDER}/img_train_tuned.results.pkl',\n","    'img_test_nontuned_results'  : f'{MODEL_FOLDER}/img_test_nontuned.results.pkl',\n","    'img_test_tuned_results'     : f'{MODEL_FOLDER}/img_test_tuned.results.pkl'\n","}"]},{"cell_type":"markdown","metadata":{"id":"6TB03yCLKTCT"},"source":["## Transfer Learning and Feature Extraction\n","\n","A key advantage of this appraoch is that you only run the base model once on your data, rather than once per epoch of training; it's a lot faster & cheaper\n","\n","<ul>\n","<li>Instantiate a base model and load pre-trained weights into it.</li>\n","<li>Run the cell image dataset through it and extract the features from an output layer; preferrably the last convalution layer.</li>\n","<li>Use that output as input data for the segmentation model.</li>\n","</ul>"]},{"cell_type":"markdown","metadata":{"id":"Q7891gydKTCT"},"source":["#### Create cleansed cell images from bounfding boxes"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1725910617783,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"XWqQgnAfKTCU"},"outputs":[],"source":["def get_annotation_files(path: str, max_files:int=0) -> list[pathlib.Path]:\n","    ''' Generates the paths to annotation files\n","    \n","    Args: \n","        path (str): the base folder containing the annotation (xml) files\n","        max_files (int): the length of the returned annotations - default (0) returns all the annotation files\n","        \n","    Returns: \n","        A list of annotation paths\n","\n","    '''\n","    \n","    \n","    annotation_files = list(pathlib.Path(path).glob('*.xml'))\n","    if len(annotation_files) == 0:\n","        raise UserWarning(\"No annotation files found - exiting process\")\n","    print(f\"There are {len(annotation_files):,} annotations in the XML folder\")\n","\n","    if max_files > 0:\n","        annotation_files = annotation_files[:max_files]\n","        print(f'User process limt set to {max_files} files')\n","\n","    return annotation_files\n","\n","def get_cell_cls_names(cell_dir: str) -> list[str]:\n","    ''' Creates a list of cell names based on the folder names\n","    \n","    Args: \n","        cell_dir (str): the parent folder containing R, G, U cell folders\n","        \n","    Returns: \n","        A list of cell folder names\n","    '''\n","    \n","    \n","    cls_names = []\n","    for sub_dir in [f for f in os.scandir(cell_dir) if f.is_dir()]:\n","        cls_names.append(sub_dir.name)\n","    return cls_names\n","\n","def parse_xml(xml_file: str, cell_cls_names: list[str]) -> tuple[NDArray, tuple[int, int]]:\n","    ''' Extracts the bounding box data from an annotation (xml) file\n","    \n","    Args: \n","         xml_file (str): a string path to an annotation file\n","         cell_cls_names list[str]: a list of the cell folder names to map the cell class to int values\n","         \n","    Returns: \n","        A tuple of array of cell boxes and dimension tuple(w x h)\n","    '''\n","    chk_limit = lambda x,y: y if x > y else x\n","\n","    cell_boxes = []\n","    tree = ET.parse(xml_file)\n","    root = tree.getroot()\n","\n","    # retrive image dimensions\n","    img_width = int(root.find('.//size/width').text) # type: ignore\n","    img_height = int(root.find('.//size/height').text) # type: ignore\n","\n","    # bounding boxes - retrieve label and co-ordinates\n","    for elem in root.findall('.//object'):\n","        cell_cls = elem.find('name').text\n","        cell_cls_idx = cell_cls_names.index(cell_cls)\n","\n","        x_min = int(elem.find('./bndbox/xmin').text)\n","        y_min = int(elem.find('./bndbox/ymin').text)\n","        x_max = int(elem.find('./bndbox/xmax').text)\n","        y_max = int(elem.find('./bndbox/ymax').text)\n","\n","        # ensure max co-ordinates do not exceed image size\n","        x_max = chk_limit(x_max, img_width)\n","        y_max = chk_limit(y_max, img_height)\n","\n","        bbox = [x_min, y_min, x_max, y_max, cell_cls_idx]\n","        cell_boxes.append(bbox)\n","\n","    return np.array(cell_boxes), (img_width, img_height)\n","\n","#\n","# Important: loaded images - shape is height, width, optionally channel(s)\n","#\n","def get_colour_image(xml_file: str) -> tuple[NDArray, str]:\n","    '''Reads a corresponding fluorescence image from an annotation file\n","    \n","    Args: \n","        xml_file: the annotation file path\n","        \n","    Returns: \n","        a numpy array representation of the respective image and the image id\n","    '''\n","    img_id = os.path.splitext(os.path.basename(xml_file))[0]\n","    img_file = f'{IMG_FLR_FOLDER}/{img_id}.tiff'\n","    img = cv2.cvtColor(cv2.imread(img_file), cv2.COLOR_BGR2RGB)\n","    return img, img_id\n","\n","def generate_cell_images(annotation_files: list[pathlib.Path], cell_dir: str, cell_resize: tuple[int, int]) -> None:\n","    \"\"\"\n","    Generates a cleaner cell from an XML annotation file and a corresponding full image.\n","\n","    Args:\n","        annotation_files list(Path): Paths to the XML annotation files.\n","        cell_dir (str): the parent folder containing R, G, U cell folders\n","        cell_resize tuple(int, int): The size of the image.\n","\n","    Returns:\n","        none\n","    \"\"\"\n","    cell_cls_names = get_cell_cls_names(cell_dir)\n","    rgb_cls_names = ['R','G','U']\n","\n","    for xml_file in annotation_files:\n","        cell_boxes, img_size = parse_xml(str(xml_file), cell_cls_names)\n","\n","        c_img, c_img_name = get_colour_image(str(xml_file))\n","        c_img = cv2.resize(c_img, img_size)\n","\n","        # Create a blank multi-channel mask\n","        cell_cnt = 0\n","        c_threshold = 5\n","\n","        for box in cell_boxes:\n","            x_min, y_min, x_max, y_max, box_cls = box\n","            cls_dir_name = cell_cls_names[box_cls]\n","\n","            # Fill the bounding box region in the mask with the corresponding color\n","            cell_img = c_img[y_min:y_max, x_min:x_max]\n","\n","            r_val = np.max(c_img[y_min:y_max, x_min:x_max, 0])\n","            g_val = np.max(c_img[y_min:y_max, x_min:x_max, 1])\n","            u_val = np.max(c_img[y_min:y_max, x_min:x_max, 2])\n","\n","            # ignore type\n","            c_pixel_idx = np.argmax([r_val, g_val, u_val])\n","            c_pixel_val = int(np.max(cell_img))\n","            c_pixel_cls = cell_cls_names.index(rgb_cls_names[c_pixel_idx])\n","\n","            # ensure the corresponding channel(color) is the most prominent in the cell region\n","            if c_pixel_val > c_threshold and c_pixel_cls == box_cls:\n","                cell_img = cv2.cvtColor(cv2.resize(cell_img, cell_resize), cv2.COLOR_RGB2BGR)\n","                fname = f'{c_img_name}_{cell_cnt}_{cls_dir_name}_{x_min}_{y_min}_{x_max}_{y_max}'\n","                cv2.imwrite(f\"{cell_dir}/{cls_dir_name}/{fname}.tiff\", cell_img)\n","                cell_cnt += 1"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1725910617784,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"NbA2YQEtKTCV"},"outputs":[],"source":["if CREATE_CELL_IMAGES:\n","    print('Creating new cell images from fluorescent full images...')\n","    files =  get_annotation_files(ANNOTATION_TRAIN_FOLDER)\n","    generate_cell_images(files, CELL_IMG_TRAIN_FOLDER, (IMG_RESIZE, IMG_RESIZE))\n","\n","    files =  get_annotation_files(ANNOTATION_TEST_FOLDER)\n","    generate_cell_images(files, CELL_IMG_TEST_FOLDER, (IMG_RESIZE, IMG_RESIZE))"]},{"cell_type":"markdown","metadata":{"id":"Vy0XosNadjWG"},"source":["#### Process new cell images\n","\n","---"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1725910617784,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"YAX34ksFKTCW"},"outputs":[],"source":["def get_cell_images(cell_dir: str, img_resize: tuple[int, int], channels:int=3, max_imgs:int=0) -> tuple[NDArray, NDArray, list[str]]:\n","    ''' Creates the input data (x) and corresponding labels (y) from a folder\n","    \n","    Args: \n","        cell_dir (str): the parent folder containing R, G, U cell folders\n","        img_resize (int, int): the cell image resized dimension\n","        channels int: the number of channels in the generated images\n","        \n","    Returns: \n","        a tuple of input cell images, corresponding labels and the class names\n","    '''\n","    cell_cls_names = []\n","    x = []\n","    y = []\n","    for sub_dir in [f for f in os.scandir(cell_dir) if f.is_dir()]:\n","        print(f'Processing {sub_dir.name} folder...')\n","        cell_cls_names.append(sub_dir.name)\n","        img_files = list(pathlib.Path(sub_dir.path).glob('*.tiff'))\n","        if max_imgs > 0:\n","            img_files = img_files[:max_imgs]\n","\n","        for img_file in img_files:\n","            img = cv2.resize(cv2.imread(str(img_file), cv2.IMREAD_GRAYSCALE), img_resize)\n","            img = np.stack((img,)*channels, axis=-1)\n","            x.append(img)\n","            y.append(len(cell_cls_names)-1)\n","\n","    print(f'Cell Classes: {cell_cls_names}')\n","\n","    x = np.array(x)\n","    y = np.stack((y,), axis=-1)\n","    y = np.array(y, dtype=np.uint8)\n","\n","    # randomise images/labels\n","    x, y = utils.shuffle(x, y) # type: ignore\n","\n","    return x, y, cell_cls_names"]},{"cell_type":"markdown","metadata":{"id":"78evsfS0KTCW"},"source":["#### Build Transfer Model using pre-trained weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76952,"status":"ok","timestamp":1725910694712,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"BlmeFAWEKTCX","outputId":"d599a44f-c385-4c1c-b190-c41df48835e5"},"outputs":[],"source":["def load_weights(model, file, desc):\n","    ''' Loads pretrained weights\n","    \n","    Args: \n","        model: a keras template model\n","        file: the corresponding file with the model's weights\n","        desc: a simple text description\n","        \n","    Returns: \n","        a model loaded with the pretrained weights\n","    \n","    '''\n","    \n","    try:\n","        model.load_weights(file)\n","        print(f'{desc} weights loaded successfully.')\n","    except Exception as e:\n","        print(f'Error loading {desc} weights:', e)\n","        raise\n","    return model\n","\n","def load_results(file, desc):\n","    ''' Loads pretrained results\n","    \n","    Args: \n","        file: the corresponding file with the model's metrics\n","        desc: a simple text description\n","        \n","    Returns: \n","        the pretrained model's metrics\n","        \n","    '''\n","    try:\n","        with open(file , 'rb') as f:\n","            results = pickle.load(f)\n","        print(f'{desc} results loaded successfully.')\n","    except Exception as e:\n","        print(f'error loading {desc} results:', e)\n","        raise\n","    return results\n","\n","def save_results(results, file, desc):\n","    ''' Saves the model's weights and metrics for future use instead of retraining it\n","    \n","    Args: \n","        results: the models's results\n","        file: the path to save the results\n","        desc: a simple text description\n","    '''\n","    try:\n","        with open(file, 'wb') as f:\n","            pickle.dump(results, f)\n","            print(f'{desc} results saved successfully.')\n","    except Exception as e:\n","        print(f'Error loading {desc} results:', e)\n","        raise\n","\n","def build_cell_model(input_dims: tuple[int, int], n_class):\n","    ''' creates a cell models\n","    \n","    Args: \n","        input_dims: the shape of the input\n","        n_class: the number of the model's output classes\n","        \n","    Returns: \n","        A cell model\n","    '''\n","    \n","    input = keras.layers.Input(shape=input_dims, name='input')\n","    x = keras.applications.vgg16.preprocess_input(input)\n","\n","    base_model = keras.applications.VGG16 (\n","        weights='imagenet',\n","        include_top=False,\n","        input_shape=input_dims\n","        )\n","    base_model.trainable = False\n","    x = base_model(input)\n","\n","    # replace fully connected layers\n","    x = keras.layers.Conv2D(filters=1024, kernel_size=3, padding=\"same\", activation='relu', kernel_initializer=\"he_normal\")(x)\n","    x = keras.layers.GlobalAveragePooling2D()(x)\n","\n","    output = keras.layers.Dense(n_class, activation='softmax', name='output')(x)\n","\n","    model = keras.Model(inputs=input, outputs=output,  name='cell_model')\n","    model.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    return model\n","\n","if RETRAIN_CELL_MODEL or not os.path.exists(model_info['cell_train_weights']):\n","    # Only load data sets if training\n","    x_cell_train, y_cell_train, cell_cls_names = get_cell_images(CELL_IMG_TRAIN_FOLDER, (IMG_RESIZE, IMG_RESIZE), max_imgs=CELL_CLS_TRAIN_CNT)\n","    print(f'Training data: X={x_cell_train.shape} Y={y_cell_train.shape}')\n","\n","    x_cell_test, y_cell_test, cell_cls_names = get_cell_images(CELL_IMG_TEST_FOLDER, (IMG_RESIZE, IMG_RESIZE), max_imgs=CELL_CLS_TEST_CNT)\n","    print(f'Test data: X={x_cell_test.shape} Y={y_cell_test.shape}')\n","\n","    # build cell model\n","    input_dims = (IMG_RESIZE, IMG_RESIZE, 3)\n","    cell_model = build_cell_model(input_dims, len(cell_cls_names))\n","\n","    # train model - save results and weights\n","    train_results = cell_model.fit(\n","        x=x_cell_train,\n","        y=y_cell_train,\n","        validation_split=VALIDATION_SPLIT,\n","        batch_size=BATCH_SIZE,\n","        epochs=EPOCHS\n","        )\n","\n","    cell_model.save_weights(model_info['cell_train_weights'])\n","    train_metrics = train_results.history\n","    save_results(train_metrics, model_info['cell_train_results'],  'cell training')\n","\n","    cell_model.summary(show_trainable=True)\n","else:\n","    x_cell_test, y_cell_test, cell_cls_names = get_cell_images(CELL_IMG_TEST_FOLDER, (IMG_RESIZE, IMG_RESIZE), max_imgs=CELL_CLS_TEST_CNT)\n","    print(f'Test data: X={x_cell_test.shape} Y={y_cell_test.shape}')\n","\n","    cell_model = build_cell_model((IMG_RESIZE, IMG_RESIZE, 3), len(cell_cls_names))\n","    if os.path.exists(model_info['cell_train_weights']):\n","        cell_model = load_weights(cell_model, model_info['cell_train_weights'], 'cell training')\n","\n","    train_metrics = load_results(model_info['cell_train_results'], 'training')"]},{"cell_type":"markdown","metadata":{"id":"0SDnm_2fKTCY"},"source":["#### Performance Metrics for cell model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":678},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1725910695420,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"MudobcjiKTCY","outputId":"b5097d31-ceff-48bf-da66-2f5ccdc19603"},"outputs":[],"source":["# Display training metrics\n","print(f\"Best Train accuracy: {max(train_metrics['accuracy']):.3f}\")\n","print(f\"Best Train loss: {min(train_metrics['loss']):.3f}\")\n","print(f\"Best validation accuarcy: {max(train_metrics['val_accuracy']):.3f}\")\n","print(f\"Best validation loss: {min(train_metrics['val_loss']):.3f}\")\n","\n","fig, ax = plt.subplots(1, 2, figsize=(9, 6), sharex=True, sharey=False)\n","ax[0].plot(train_metrics['loss'], label='Train Loss')\n","ax[0].plot(train_metrics['val_loss'], label='Validation Loss')\n","ax[0].set(xlabel='Epoch', ylabel='Loss')\n","ax[1].plot(train_metrics['accuracy'], label='Train Accuracy')\n","ax[1].plot(train_metrics['val_accuracy'], label='Validation Accuracy')\n","ax[1].set(xlabel='Epoch', ylabel='Accuracy')\n","\n","plt.legend(['Train', 'Validation'])\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":645},"executionInfo":{"elapsed":41790,"status":"ok","timestamp":1725910737199,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"yq6yAqodKTCY","outputId":"c85a843f-2f65-47e7-ed4f-2536b4566ecb"},"outputs":[],"source":["# get predictions\n","y_cell_pred = cell_model.predict(x_cell_test)\n","y_cell_pred = np.argmax(y_cell_pred, axis=1)\n","\n","# show accuracy scores for class\n","labels = cell_cls_names\n","labels_idx = [i for i,e in enumerate(labels)]\n","print(classification_report(y_cell_test, y_cell_pred, digits=4, labels=labels_idx, target_names=labels))\n","\n","# Print confusion matrix\n","conf_matrix = ConfusionMatrixDisplay.from_predictions(y_cell_test, y_cell_pred, colorbar=False, display_labels=labels)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NlsydOTZKTCZ"},"source":["### Image Segmentation and Classification"]},{"cell_type":"markdown","metadata":{"id":"H4INDch0KTCZ"},"source":["#### Load training data and pre-process full images"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1725910737201,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"iIYCmkN4KTCZ"},"outputs":[],"source":["def display_images(display_list: list[NDArray], title: list[str]) -> None:\n","    ''' visualizes a list of images using matplotlib\n","    \n","    Args: \n","        display_list list[NDArray]: a list of images\n","        title list[str]: a list of string used as titles for the plots\n","    '''\n","    \n","    plt.figure(figsize=(15, 15))\n","\n","    for i in range(len(display_list)):\n","        plt.subplot(1, len(display_list), i+1)\n","        plt.title(title[i])\n","        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n","        plt.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Important: loaded images - shape is height, width, optionally channel(s)\n","def get_bw_image(xml_file: str) -> tuple[NDArray, str] :\n","    ''' Creates a grayscale image\n","    \n","    Args: \n","        xml_file str: the annotation file path\n","    \n","    Returns: \n","        a numpy array representation of the respective image and the image id\n","        \n","    '''\n","    \n","    img_id = os.path.splitext(os.path.basename(xml_file))[0]\n","    img_file = f'{IMG_BF_FOLDER}/{img_id}.tiff'\n","    img = cv2.imread(img_file, cv2.IMREAD_GRAYSCALE)\n","    return img, img_id\n","\n","def generate_rgb_mask(annotation_files: list[pathlib.Path], cell_cls_names: list[str], img_resize: tuple[int, int]) -> NDArray:\n","    '''Creates an RGB based mask from annotation bounding boxes\n","    \n","    Args: \n","        annotation_files list[Path]: a list of annotation file paths\n","        cell_cls_names list[str]: a list of the cell folder names to map the cell class to int values\n","        img_resize (int, int): the cell image resized dimension \n","        \n","    Returns: \n","        An array of masks\n","    '''\n","    masks = []\n","    c_threshold = 1\n","    rgb_cls_names = ['R','G','U']\n","\n","    for xml_file in annotation_files:\n","        # print(xml_file)\n","        # get the annotation data\n","        cell_boxes, img_size = parse_xml(str(xml_file), cell_cls_names)\n","\n","        # correct wrong metadata\n","        w = 540 if img_size[0] > 540 else img_size[0]\n","        h = 540 if img_size[1] > 540 else img_size[1]\n","        img_size = (w,h)\n","\n","        c_img,_ = get_colour_image(str(xml_file))\n","        c_img = cv2.resize(c_img, img_size)\n","\n","        # assign labels to each pixel\n","        # note: image format is height x width\n","        mask = np.zeros((c_img.shape[0], c_img.shape[1], 3), dtype=np.uint8)\n","\n","        for box in cell_boxes:\n","            x_min, y_min, x_max, y_max, box_cls = box\n","            if x_max > img_size[0]:\n","              x_max = img_size[0]\n","            if y_max > img_size[1]:\n","              y_max = img_size[1]\n","\n","            # for each pixel, get most intense colour.\n","            # only assign class to pixel if class matches\n","            # image format - height x width so lead with y\n","            for h in range(y_min-1, y_max-1):\n","                for w in range(x_min-1, x_max-1):\n","                    # get the max pixel index(channel)\n","                    pixel_r_max_val = np.max(c_img[h, w, 0])\n","                    pixel_g_max_val = np.max(c_img[h, w, 1])\n","                    pixel_u_max_val = np.max(c_img[h, w, 2])\n","\n","                    c_pixel_idx = np.argmax([pixel_r_max_val, pixel_g_max_val, pixel_u_max_val])\n","                    c_pixel_val = np.max([pixel_r_max_val, pixel_g_max_val, pixel_u_max_val])\n","                    c_pixel_cls = cell_cls_names.index(rgb_cls_names[c_pixel_idx])\n","\n","                    # check that the pixel max index matches the box class\n","                    # assumes background is zero.\n","                    if c_pixel_cls == box_cls and c_pixel_val > c_threshold:\n","                        mask[h, w, c_pixel_idx] = 255\n","\n","        mask = cv2.resize(mask, img_resize)\n","        masks.append(mask)\n","\n","    return  np.array(masks)\n","\n","def generate_dataset(annotation_files: list[pathlib.Path], cell_cls_names: list[str], img_resize: tuple[int, int]) -> tuple[NDArray, NDArray, NDArray]:\n","    '''Creates an RGB based mask from annotation bounding boxes\n","    \n","    Args: \n","        annotation_files list[Path]: a list of annotation file paths\n","        cell_cls_names list[str]: a list of the cell folder names to map the cell class to int values\n","        img_resize (int, int): the cell image resized dimension \n","        \n","    Returns: \n","        An tuple of grayscale images, corresponding masks and a colored version\n","    '''\n","    \n","    bw_imgs = []\n","    c_imgs=[]\n","    masks = []\n","    c_threshold = 1\n","    rgb_cls_names = ['R','G','U']\n","\n","    for xml_file in annotation_files:\n","        # print(xml_file)\n","        # get the annotation data\n","        cell_boxes, img_size = parse_xml(str(xml_file), cell_cls_names)\n","\n","        # correct wrong metadata\n","        w = 540 if img_size[0] > 540 else img_size[0]\n","        h = 540 if img_size[1] > 540 else img_size[1]\n","        img_size = (w,h)\n","\n","        # get associated brightfield/fluorescent images\n","        # resize to annotated size\n","        bw_img,_ = get_bw_image(str(xml_file))\n","        bw_img = cv2.resize(bw_img, img_size)\n","\n","        c_img,_ = get_colour_image(str(xml_file))\n","        c_img = cv2.resize(c_img, img_size)\n","\n","        # assign labels to each pixel\n","        # note: image format is height x width\n","        mask = np.zeros((bw_img.shape[0], bw_img.shape[1]), dtype=np.uint8)\n","\n","        for box in cell_boxes:\n","            x_min, y_min, x_max, y_max, box_cls = box\n","            if x_max > img_size[0]:\n","              x_max = img_size[0]\n","            if y_max > img_size[1]:\n","              y_max = img_size[1]\n","\n","            # for each pixel, get most intense colour.\n","            # only assign class to pixel if class matches\n","            # image format - height x width so lead with y\n","            for h in range(y_min-1, y_max-1):\n","                for w in range(x_min-1, x_max-1):\n","                    # get the max pixel index(channel)\n","                    pixel_r_max_val = np.max(c_img[h, w, 0])\n","                    pixel_g_max_val = np.max(c_img[h, w, 1])\n","                    pixel_u_max_val = np.max(c_img[h, w, 2])\n","\n","                    c_pixel_idx = np.argmax([pixel_r_max_val, pixel_g_max_val, pixel_u_max_val])\n","                    c_pixel_val = np.max([pixel_r_max_val, pixel_g_max_val, pixel_u_max_val])\n","                    c_pixel_cls = cell_cls_names.index(rgb_cls_names[c_pixel_idx])\n","\n","                    # check that the pixel max index matches the box class\n","                    # assumes background is zero.\n","                    if c_pixel_cls == box_cls and c_pixel_val > c_threshold:\n","                        mask[h, w] = c_pixel_cls\n","\n","\n","        # resize and convert to 3 channel for vgg16\n","        bw_img = cv2.resize(bw_img, img_resize)\n","        bw_img = np.stack((bw_img,)*3, axis=-1)\n","\n","        # resize and convert to 1 channel for vgg model 16\n","        mask = cv2.resize(mask, img_resize)\n","        mask = np.stack((mask,), axis=-1)\n","\n","        # just resize colour image\n","        c_img = cv2.resize(c_img, img_resize)\n","\n","        # save processed data\n","        bw_imgs.append(bw_img)\n","        masks.append(mask)\n","        c_imgs.append(c_img)\n","\n","    return np.array(bw_imgs), np.array(masks), np.array(c_imgs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"executionInfo":{"elapsed":2346204,"status":"ok","timestamp":1725916835473,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"-a3_48ahKTCa","outputId":"775a6bdb-bd1a-467c-e436-c2a9e4e275e5"},"outputs":[],"source":["# add background avoid negative bias\n","img_cls_names = ['Background'] + cell_cls_names\n","print(f'Image classes: {img_cls_names}')\n","\n","# load training/validation dataset (if needed)\n","files = get_annotation_files(ANNOTATION_TRAIN_FOLDER, IMG_TRAIN_CNT)\n","x_img_train, y_img_train, x_img_train_colour = generate_dataset(files[:5], img_cls_names, (IMG_RESIZE, IMG_RESIZE))\n","print(x_img_train.shape, y_img_train.shape, x_img_train_colour.shape)\n","\n","title = ['Input Image(Brightfield)', 'Input Image(Fluorescence)', 'True Mask', 'RGB Mask']\n","display_images([x_img_train[0], x_img_train_colour[0], y_img_train[0], generate_rgb_mask(files[:1], img_cls_names, (IMG_RESIZE, IMG_RESIZE))[0]], title)\n","\n","# load test dataset\n","files =  get_annotation_files(ANNOTATION_TEST_FOLDER, IMG_TEST_CNT)\n","x_img_test, y_img_test, x_img_test_colour = generate_dataset(files[1], img_cls_names, (IMG_RESIZE, IMG_RESIZE))\n","print(x_img_test.shape, y_img_test.shape, x_img_test_colour.shape)\n","\n","img_data = {\n","    'x_train': x_img_train,\n","    'y_train': y_img_train,\n","    'x_test': x_img_test,\n","    'y_test': y_img_test,\n","    'x_train_colour': x_img_train_colour,\n","    'x_test_colour': x_img_test_colour\n","}"]},{"cell_type":"markdown","metadata":{"id":"OlBVZil3KTCa"},"source":["#### Train Segmentation Model"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1725916835476,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"_MTBm37JKTCa"},"outputs":[],"source":["def build_img_model(input_dims: tuple[int, int], n_class: int, vanilla: bool):\n","    ''' Builds an image segmentation model\n","    \n","    Args: \n","        input_dims tuple(int, int): the shape of the model's input\n","        n_class int: the number of the classes of the model's output layer\n","        vanilla bool: a flag to specify when to use a vanilla version of the VGG16 pretrained model\n","        \n","    Returns: \n","        a keras based image segmentation model\n","    '''\n","    \n","    inputs = keras.layers.Input(shape=input_dims, name='input')\n","    x = keras.applications.vgg16.preprocess_input(inputs)\n","\n","    # pre-processing steps\n","    x = keras.layers.RandomFlip('horizontal', name='horizontal_flip')(x)\n","    x = keras.layers.RandomRotation(0.1, name='rotation')(x)\n","\n","    # retrieve layers from cell/transfer model\n","    cell_model = build_cell_model(input_dims, len(cell_cls_names))\n","    if not vanilla:\n","        cell_model = load_weights(cell_model, model_info['cell_train_weights'], 'Cell Model')\n","    base_model = cell_model.get_layer('vgg16')\n","\n","    # save final conv layer in each block for back propagation\n","    residuals=[]\n","    filters = []\n","    conv_layer =None\n","    for layer in base_model.layers[1:]:\n","        layer.trainable = False\n","        x = layer(x)\n","        if isinstance(layer, keras.layers.MaxPooling2D):\n","            residuals.append(conv_layer)\n","            filters.append(conv_layer.shape[3])\n","        if isinstance(layer, keras.layers.Conv2D):\n","            conv_layer = x\n","\n","    # final conv layer has no max pooling so need for back propagation\n","    for layer in cell_model.layers:\n","        if isinstance(layer, keras.layers.Conv2D):\n","            layer.trainable = True\n","            x = layer(x)\n","\n","    # add upsample layers\n","    for filter in reversed(filters):\n","        # Transpose convolution\n","        x = keras.layers.Conv2DTranspose(filter, kernel_size=2, strides=2, activation='relu', padding=\"same\", kernel_initializer=\"he_normal\")(x)\n","        x = keras.layers.concatenate([x, residuals.pop()])\n","        # Two convolutions\n","        x = keras.layers.Conv2D(filter, kernel_size=3, activation='relu', padding=\"same\", kernel_initializer=\"he_normal\")(x)\n","        x = keras.layers.Conv2D(filter, kernel_size=3, activation='relu', padding=\"same\", kernel_initializer=\"he_normal\")(x)\n","        if filter > 128:\n","            x = keras.layers.Conv2D(filter, kernel_size=3, activation='relu', padding=\"same\", kernel_initializer=\"he_normal\")(x)\n","\n","    # Output layer\n","    outputs = keras.layers.Conv2D(filters=n_class, kernel_size=1, activation=\"softmax\", name='per-pixel_clsf')(x)\n","\n","    # Create intersection over union metric\n","    seg_iou = tf.keras.metrics.MeanIoU(num_classes=n_class, sparse_y_true=True, sparse_y_pred=False, name='iou')\n","\n","    # Compile the model\n","    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='segmentation_model')\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[seg_iou])\n","\n","    return model\n","\n","def train_img_model(model, data, val_split, epochs, batch_size, w_file, r_file, desc):\n","    ''' A custom function to train the image segmentation model and save the results\n","    \n","    Args: \n","        model: the segmentation model\n","        val_split: the fraction of the input used for validation\n","        epochs: the number of times the models trains with all the input data\n","        batch_size: the number of input data to pass to the model at a time\n","        w_file: the file path to the saved model results\n","        r_file: the file path to save the model results\n","    '''\n","\n","    model_chkptr = tf.keras.callbacks.ModelCheckpoint(\n","            w_file,\n","            monitor='loss',\n","            verbose=0,\n","            save_best_only=True,      # save trained weights (best values)\n","            save_weights_only=True,   #\n","            mode='min',\n","            save_freq='epoch')\n","\n","    model_earlystp = tf.keras.callbacks.EarlyStopping(\n","        monitor=\"val_loss\",\n","        min_delta=0.001,\n","        patience=10,\n","        verbose=0,\n","        mode=\"min\",\n","        restore_best_weights=True)\n","\n","    # Fit the model (best weights saved)\n","    results = model.fit(\n","        x=data['x_train'],\n","        y=data['y_train'],\n","        epochs=epochs,\n","        validation_split=val_split,\n","        batch_size=batch_size,\n","        callbacks=[model_chkptr,model_earlystp],\n","        verbose=2\n","        )\n","\n","    # Save results\n","    results = results.history\n","    save_results(results, r_file, desc)\n","\n","    return results\n","\n","def do_training(data, w_file, r_file, cls_names: list[str], vanilla:bool=False):\n","    ''' Performs the models training\n","    \n","    Args: \n","        data NDArray: a numpy representation of the input images and their masks\n","        w_file: the file path to the saved model results\n","        r_file: the file path to save the model results\n","        cls_names list(str): a list of the cell class labels\n","        vanilla bool:  a flag to use tune or untuned model\n","\n","    '''\n","\n","    # process description\n","    desc = 'non-tuned' if vanilla else 'tuned'\n","    desc = f'Segmentation training ({desc} transfer model)'\n","\n","    # define model\n","    img_model = build_img_model((IMG_RESIZE, IMG_RESIZE, 3), len(cls_names), vanilla) # type: ignore\n","\n","    # If we have previous weights, training is not necessary\n","    if RETRAIN_IMG_MODEL or not (os.path.exists(w_file) and os.path.exists(r_file)):\n","        metrics = train_img_model(img_model, data, VALIDATION_SPLIT, EPOCHS, BATCH_SIZE, w_file, r_file, desc)\n","    else:\n","        # we still need to reload weights/metrics\n","        load_weights(img_model, w_file, desc)\n","        metrics = load_results(r_file, desc)\n","\n","    return img_model, metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":920909,"status":"ok","timestamp":1725917756380,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"sMy_R0bPKTCb","outputId":"1375b21a-5706-4dbd-a663-fa98509d3cfd"},"outputs":[],"source":["# perform segmentation using non-tuned transfer models\n","img_model1, train_m1 = do_training(\n","    img_data,\n","    w_file = model_info['img_train_nontuned_weights'],\n","    r_file = model_info['img_train_nontuned_results'],\n","    cls_names = img_cls_names,\n","    vanilla = True\n","    )\n","print(f'Segmentation Mode (non-tuned transfer model) - training metrics...\\n{train_m1}\\n')\n","img_model1.summary()\n","\n","# perform segmentation using tuned transfer models\n","img_model2, train_m2 = do_training(\n","    img_data,\n","    w_file = model_info['img_train_tuned_weights'],\n","    r_file = model_info['img_train_tuned_results'],\n","    cls_names = img_cls_names\n","    )\n","print(f'Segmentation Mode (non-tuned transfer model) - training metrics...\\n{train_m2}\\n')"]},{"cell_type":"markdown","metadata":{"id":"f6Xz4SO-KTCb"},"source":["#### Performance Metrics for segmentation model"]},{"cell_type":"markdown","metadata":{"id":"E__J4RzOKTCb"},"source":["<p>\n","Pixel accuracy metric calculates the percentage of pixels that were correctly classified according to the segmentation mask. Unfortunately, with very sparse class representations the pixel accuracy will be biased towards negative cases. We have addressed thsi issue by assigning a class to the background as well as opting for mean intersection over union (IOU). For our ground truth and predicted segmenation masks, we count the number of pixels that overlap and divide by the total area of both masks; value will be 0-1, the higher the value, the better.\n","</p>\n","<p>&emsp;IOU = <u>number of overlapping pixels (intersection)</u></br>&emsp;&emsp;&emsp;&emsp;True area + Predicted area âˆ’ intersection</p>\n","\n","Dice coefficient will the same answer but maybe not the same value; we will use IOU."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":14619,"status":"ok","timestamp":1725917770998,"user":{"displayName":"Doug C","userId":"03753993097516619119"},"user_tz":-60},"id":"MdeitbFnKTCc","outputId":"1bf1e014-d461-477b-a27c-38d5d6203c06"},"outputs":[],"source":["\n","def evaluate_model(model, data: NDArray, r_file: str, r_desc: str, vanilla: bool=False):\n","    ''' A custom function to handle the model's evaluation\n","    '''\n","\n","    if RETRAIN_IMG_MODEL or not os.path.exists(r_file):\n","        metrics = model.evaluate(x=data['x_test'], y=data['y_test'])\n","        save_results(metrics, r_file, r_desc)\n","    else:\n","        metrics = load_results(r_file, r_desc)\n","\n","    return metrics\n","\n","# plot loss and IOU metrics\n","def display_metrics(train_m, test_m, title):\n","    ''' Shows the metrics of the trained/loaded model\n","    '''\n","\n","    print(f'{title}...')\n","    print(f\"Best Train loss: {min(train_m['loss']):.3f}\")\n","    print(f\"Best Train seg iou: {max(train_m['iou']):.3f}\")\n","    print(f\"Best validation loss: {min(train_m['val_loss']):.3f}\")\n","    print(f\"Best validation seg iou: {max(train_m['val_iou']):.3f}\")\n","    print(f\"Test loss: {test_m[0]:.3f}\")\n","    print(f\"Test seg iou: {test_m[1]:.3f}\")\n","\n","    _, ax = plt.subplots(1, 2, figsize=(8, 6), sharex=True, sharey=False)\n","    # fig.suptitle(title)\n","\n","    ax[0].plot(train_m['loss'], label='Train Loss')\n","    ax[0].plot(train_m['val_loss'], label='Validation Loss')\n","    ax[0].scatter(len(train_m['loss']), test_m[0], label='Test Loss', color='green')\n","    ax[0].set(xlabel='Epoch', ylabel='Loss')\n","\n","    ax[1].plot(train_m['iou'], label='Train IOU')\n","    ax[1].plot(train_m['val_iou'], label='Validation IOU')\n","    ax[1].scatter(len(train_m['iou']), test_m[1], label='Test IoU', color='green')\n","    ax[1].set(xlabel='Epoch', ylabel='Seg IOU')\n","\n","    plt.legend(['Train', 'Validation'])\n","    plt.tight_layout()\n","    plt.show()\n","\n","# perform evaluation and display results for tuned/untuned transfer models\n","desc = 'Segmenation Metrics (non-tuned transfer model)'\n","test_m1 = evaluate_model(img_model1, img_data, model_info['img_test_nontuned_results'], desc, vanilla=True)\n","display_metrics(train_m1, test_m1, desc)\n","\n","# perform evaluation and display results for tuned/untuned\n","desc = 'Segmenation Metrics (tuned transfer model)'\n","test_m2 = evaluate_model(img_model2, img_data, model_info['img_test_tuned_results'], desc)\n","display_metrics(train_m2, test_m2, desc)"]},{"cell_type":"markdown","metadata":{},"source":["##### Testing the Segmentation Models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_img_pred1 = img_model1.predict(x_img_test)\n","y_img_pred2 = img_model2.predict(x_img_test)\n","\n","y_img_pred_classes1 = [np.argmax(pred, -1) for pred in y_img_pred1]\n","y_img_pred_classes2 = [np.argmax(pred, -1) for pred in y_img_pred2]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_, ax = plt.subplots(1, 3, figsize=(15, 15));\n","ax = ax.ravel()\n","\n","ax[0].imshow(y_img_test[0]);\n","ax[0].set_title('True Mask');\n","ax[0].axis('off');\n","ax[1].imshow(y_img_pred_classes1[0]);\n","ax[1].set_title('Non-tuned Weights');\n","ax[1].axis('off');\n","ax[2].imshow(y_img_pred_classes2[0]);\n","ax[2].set_title('Tuned Weights');\n","ax[2].axis('off');"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
